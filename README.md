# awesome-robotic-manipulation
This repository collects the studies on robotic manipulation.

### Similar Work
- ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills <br>
  [Paper](https://arxiv.org/abs/2302.04659) / [Code](https://github.com/haosulab/ManiSkill2)
- RT-1: Robotics Transformer for real-world control at scale <br>
  [Paper](https://arxiv.org/abs/2212.06817) / [Code](https://github.com/google-research/robotics_transformer) / [Project Page](https://robotics-transformer.github.io/)
- ChatGPT for Robotics: Design Principles and Model Abilities <br>
  [Paper](https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf) / [Code](https://github.com/microsoft/PromptCraft-Robotics) / [Project Page](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/chatgpt-for-robotics/)
- Open-World Object Manipulation using Pre-Trained Vision-Language Models <br>
  [Paper](https://robot-moo.github.io/assets/moo.pdf) / [Project Page](https://robot-moo.github.io/)
- PaLM-E: An Embodied Multimodal Language Model <br>
  [Paper](https://arxiv.org/abs/2303.03378) / [Project Page](https://palm-e.github.io/) 
- Scaling Robot Learning with Semantically Imagined Experience <br>
  [Paper](https://arxiv.org/abs/2302.11550) / [Project Page](https://diffusion-rosie.github.io/)
- Text2Motion: From Natural Language Instructions to Feasible Plans <br>
  [Paper](https://arxiv.org/abs/2303.12153) / [Project Page](https://sites.google.com/stanford.edu/text2motion)

### References
- Graph Inverse Reinforcement Learning from Diverse Videos <br>
  [CoRL 2022 Oral](https://arxiv.org/abs/2207.14299) / [Code](https://github.com/SateeshKumar21/graph-inverse-rl) / [Project Page](https://sateeshkumar21.github.io/GraphIRL/)
- DexMV: Imitation Learning for Dexterous Manipulation from Human Videos <br>
  [ECCV 2022](https://arxiv.org/abs/2108.05877) / [Code](https://github.com/yzqin/dexmv-sim) / [Project Page](https://yzqin.github.io/dexmv/)
- VIMA: General Robot Manipulation with Multimodal Prompts <br>
  [Paper](https://arxiv.org/abs/2210.03094) / [Code](https://github.com/vimalabs/VIMA) / [Project](https://vimalabs.github.io/)

